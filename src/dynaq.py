import numpy as np
import random
from collections import defaultdict
import trackio as trackio

from src.models import TransitionCountsModel

class DynaQAgent:
    """
    Tabular Dyna-Q agent with an ensemble of Q-tables.

    This agent learns directly from real environment interactions (model-free updates)
    and from simulated experiences generated by a learned transition model (planning).
    The ensemble of Q-tables is used to reduce bias in target estimates.

    Attributes:
        env: Environment with discrete action space (must support .step() and .action_space).
        model: Transition model (TransitionCountsModel) for storing and sampling experience.
        lr: Learning rate for Q-updates.
        gamma: Discount factor.
        epsilon: Initial epsilon for epsilon-greedy policy.
        epsilon_decay: Multiplicative decay applied to epsilon after each environment step.
        epsilon_min: Minimum epsilon value.
        planning_steps: Number of model-generated updates per real step after warmup.
        ensemble_size: Number of independent Q-tables in the ensemble.
        q_table: List of dicts mapping state → np.array of Q-values, one per ensemble member.
        warmup_steps: Number of real steps before planning is activated.
        env_steps: Counter of environment steps taken.

    Methods:
        choose_action(state): Epsilon-greedy action selection over the ensemble.
        update_q_table(state, action, reward, next_state, done): Q-learning update for one ensemble member.
        learn(obs): Executes one full Dyna-Q iteration: act, update from real experience, 
                    update model, plan from simulated experience, decay epsilon.
    """

    def __init__(
        self, 
        env, 
        learning_rate=0.1, 
        discount_factor=0.99,
        epsilon=1.0,
        epsilon_decay=0.9995,
        epsilon_min=0.05,
        planning_steps=10,
        ensemble_size=5,
        warmup_steps=5_000,
        ):
        
        self.env = env
        self.model = TransitionCountsModel()
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.planning_steps = planning_steps

        self.ensemble_size = ensemble_size
        self.q_table = [defaultdict(lambda: np.zeros(env.action_space.n)) for _ in range(self.ensemble_size)]

        self.warmup_steps = warmup_steps
        self.env_steps = 0

    def choose_action(self, state):
        """
        Select an action using an epsilon-greedy policy over the ensemble.

        The ensemble's Q-values for `state` are averaged across members, and
        the action with the highest mean value is chosen with probability
        1 - epsilon; otherwise, a random action is chosen.

        Args:
            state: Hashable representation of the environment state.

        Returns:
            int: Chosen action index.
        """
        if random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            return np.argmax(np.sum([self.q_table[i][state] for i in range(self.ensemble_size)], axis=0))

    def update_q_table(self, state, action, reward, next_state, done=False):
        """
        Perform a Q-learning update for a randomly chosen ensemble member.

        If `done` is True, uses the reward as the target (no bootstrapping);
        otherwise uses the maximum over Q-values at `next_state` as the target.

        Args:
            state: Current state.
            action (int): Action taken.
            reward (float): Reward received.
            next_state: Next observed state.
            done (bool): Whether the episode terminated.
        """
        # Sample an ensemble member to update
        k = random.choice(range(self.ensemble_size))

        if done:
            td_target = reward
        else:
            # Greedy action for the chosen member k
            next_action = np.argmax(self.q_table[k][next_state])

            # Use the average Q-value from other ensemble members for the target
            # This reduces bias from the member being updated.
            other_members_q_values = [self.q_table[i][next_state][next_action] for i in range(self.ensemble_size) if i != k]
            next_state_q = np.mean(other_members_q_values) if other_members_q_values else 0
        
            td_target = reward + self.gamma * next_state_q

        # Q-learning update
        td_error = td_target - self.q_table[k][state][action]
        self.q_table[k][state][action] += self.lr * td_error

    def learn(self, obs):
        """
        Execute a full Dyna-Q update cycle for the current observation.

        Steps:
        1. Choose action via epsilon-greedy.
        2. Step environment to obtain reward and next state.
        3. Perform real Q-learning update from experience.
        4. Store transition in model.
        5. Perform planning updates from model samples.
        6. Decay epsilon.

        Args:
            obs: Current observation/state from environment.

        Returns:
            tuple: (next_obs, reward, terminated, truncated, info) from env.step().
        """
        state = tuple(obs.flatten())

        # 1. Act
        action = self.choose_action(state)
        next_obs, reward, terminated, truncated, info = self.env.step(action)
        next_state = tuple(next_obs.flatten())

        # 2. Direct RL Update
        done = terminated or truncated
        self.update_q_table(state, action, reward, next_state, done)

        # 3. Model Learning
        self.model.learn(state, action, reward, next_state)

        # 4. Planning
        if self.env_steps > self.warmup_steps:
            for _ in range(self.planning_steps):
                # Sample a random state and action from the model
                s_plan, a_plan = self.model.sample()
                # Get the predicted reward and next state from the model
                r_plan, s_prime_plan = self.model.predict(s_plan, a_plan)
                # Update Q-table with simulated experience
                self.update_q_table(s_plan, a_plan, r_plan, s_prime_plan)
            
        # 5. Decay Epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        self.env_steps += 1

        return next_obs, reward, terminated, truncated, {}


class DynaQBAgent(DynaQAgent):
    """
    Dyna-Q agent with Beta-scheduled mellowmax backups (UQL-inspired).
    (Paper: https://arxiv.org/pdf/2110.14818)

    This variant replaces the hard max in the value target with the mellowmax
    operator, using an adaptively computed inverse temperature β(s) from an 
    ensemble of Q-tables. β(s) is found by solving E[V_β(s)] = max_a E[Q(s,a)],
    optionally scaled by a factor κ.

    Additional Attributes:
        kappa: Scaling factor applied to β before use in updates.
        beta_min, beta_max: Search bounds for β.
        beta_search_max_iter: Maximum iterations for binary search over β.
        beta_tol: Tolerance for binary search termination.
        beta_history: List storing β values found during training for analysis.

    Private Methods:
        __mellowmax(Q_values, beta, pi0): Computes mellowmax for a given Q vector,
                                          inverse temperature β, and prior π₀.
    Methods:
        find_beta(state): Root-finds β(s) such that E[V_β(s)] = max_a μ_a(s).
        update_q_table(...): Uses β-scheduled mellowmax target for Q-learning update.
    """

    def __init__(
        self,
        env,
        learning_rate=0.1,
        discount_factor=0.99,
        epsilon=1.0,
        epsilon_decay=0.9995,
        epsilon_min=0.05,
        planning_steps=10,
        ensemble_size=5,
        warmup_steps=5_000,
        kappa=1.0,
        beta_search_range=(1e-20, 2e6),
        beta_search_max_iter=35,
        beta_tol=1e-6,
    ):
        super().__init__(
            env,
            learning_rate,
            discount_factor,
            epsilon,
            epsilon_decay,
            epsilon_min,
            planning_steps,
            ensemble_size,
            warmup_steps,
        )
        self.kappa = kappa
        self.beta_min, self.beta_max = beta_search_range
        self.beta_search_max_iter = beta_search_max_iter
        self.beta_tol = beta_tol

        # For logging purposes
        self.beta_history = []

    def __mellowmax(self, Q_values, beta, pi0):
        """
        Compute the mellowmax value for a given Q-vector.

        Implements:
            V_beta(s) = (1/beta) * log( sum_a pi0(a) * exp(beta * Q(s,a)) )

        Args:
            Q_values (np.ndarray): Array of Q-values for each action.
            beta (float): Inverse temperature.
            pi0 (np.ndarray): Prior probability distribution over actions.

        Returns:
            float: Soft maximum of Q-values.
        """
        if abs(beta) < 1e-12:  # β≈0 → expectation under pi0
            return np.sum(pi0 * Q_values)

        log_pi0 = np.log(pi0)
        arr = log_pi0 + beta * Q_values  # log π0(a) + β Q(s,a)

        max_a = np.max(arr)
        lse = max_a + np.log(np.exp(arr - max_a).sum())

        return lse / beta

    def find_beta(self, state):
        """
        Solve for β(s) such that:
            E[V_beta(s)] = max_a E[Q(s,a)]
        where the expectation is over ensemble members.

        Uses binary search in [beta_min, beta_max], stopping when the
        function difference is within beta_tol or iteration cap is reached.

        Args:
            state: Environment state for which to compute β.

        Returns:
            float: Inverse temperature β for this state.
        """
        n_actions = self.env.action_space.n
        pi0 = np.ones(n_actions) / n_actions

        # collect ensemble Q-values for this state: shape (K, A)
        Qs = np.stack([self.q_table[i][state] for i in range(self.ensemble_size)], axis=0)  # KxA

        # mean over ensemble of mellowmax_i(β)
        def f_of_beta(beta):
            V_i = np.array([self.__mellowmax(Qs[i], beta, pi0) for i in range(self.ensemble_size)])
            E_V = V_i.mean()
            mean_Q_per_action = Qs.mean(axis=0)
            max_of_mean_Q = mean_Q_per_action.max()
            return E_V - max_of_mean_Q

        low = float(self.beta_min)
        high = float(self.beta_max)

        f_low = f_of_beta(self.beta_min)
        f_high = f_of_beta(self.beta_max)

        # If already positive at low, return low; if still negative at high, return high.
        if f_low >= 0:
            return low
        if f_high <= 0:
            return high

        # binary search
        for _ in range(self.beta_search_max_iter):
            mid = (low + high) / 2.0
            f_mid = f_of_beta(mid)
            if abs(f_mid) < self.beta_tol:
                return mid
            if f_mid > 0:
                high = mid
            else:
                low = mid

        return (low + high) / 2.0

    def update_q_table(self, state, action, reward, next_state, done=False):
        """
        Perform a Q-learning update using β-scheduled mellowmax target.

        β(s') is computed from the ensemble for the next state and scaled by κ.

        Args:
            state: Current state.
            action (int): Action taken.
            reward (float): Reward received.
            next_state: Next observed state.
            done (bool): Whether the episode terminated.
        """

        # Sample an ensemble member to update
        k = random.choice(range(self.ensemble_size))

        if done:
            td_target = reward
        else:
            # compute β(s') from ensemble (state-dependent)
            beta = self.find_beta(next_state)
            self.beta_history.append(beta)

            # effective beta after correction constant κ
            eff_beta = self.kappa * beta

            n_actions = self.env.action_space.n
            pi0 = np.ones(n_actions) / n_actions

            Qk_next = self.q_table[k][next_state]

            # compute mellowmax V for member k using eff_beta
            V_k = self.__mellowmax(Qk_next, eff_beta, pi0)

            td_target = reward + self.gamma * V_k

        td_error = td_target - self.q_table[k][state][action]
        self.q_table[k][state][action] += self.lr * td_error

